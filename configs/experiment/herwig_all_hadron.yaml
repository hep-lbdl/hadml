# @package _global_

# start the environment
# conda-start torch

# to execute this experiment run:
# python hadml/train.py experiment=herwig_all_hadron
## to add a logger
# python hadml/train.py experiment=herwig_all_hadron logger=wandb

## with training techniques
# python hadml/train.py experiment=herwig_all_hadron logger=wandb +trainer.gradient_clip_val=0.5

defaults:
  - override /datamodule: herwig.yaml
  - override /model: cond_particle_gan_separate_embedding.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
task_name: "herwigAllhadron"
tags: ["herwig", "allhadrons"]
# ckpt_path: /global/cfs/cdirs/m3246/jay/hadml/logs/herwigAllhadron/runs/2023-08-18_17-23-33/lightning-hydra-template/2sq8mqmt/checkpoints/last.ckpt
# train: False

seed: 12345

trainer:
  max_epochs: 15
  val_check_interval: 0.5

callbacks:
  model_checkpoint:
    monitor: val/swd
    # monitor: "val/min_avg_swd"
    mode: "min"
    save_top_k: 5
    save_last: True
  
## override /datamodule:
datamodule:
  batch_size: 40_000
  pin_memory: True
  core_dataset:
    train_val_test_split: [ 0.97, 0.005, 0.025 ]
    frac_data_used: 1.0
    num_used_hadron_types: 40
    fname: allHadrons_with_quark_flavour.dat



## override /model:
model:
  noise_dim: 64
  # loss_type: "wasserstein" ## "wasserstein" or "bce" or "ls"
  loss_type: "bce"
  wasserstein_reg: 0.0
  r1_reg: 200.0

  ## how often the optimizers will be used.
  num_critics: 1
  num_gen: 1

  generator:
    input_dim: 70   # ${model.noise_dim} + ${model.cond_info_dim}
    hidden_dims: ${gen_list:1000,4} # = [256, 256]

  discriminator:
    hidden_dims: ${gen_list:1000,4} # = [256, 256]
    dropout: 0
    input_dim: 88
  
  
  # criterion:
  #   __target__: torch.nn.BCELoss
  #   reduction: "mean"

  # optimizer_generator:
  #   lr: 0.000001

  # optimizer_discriminator:
  #   lr: 0.000005


#   scheduler_generator:
#   # _target_: torch.optim.lr_scheduler.ExponentialLR
#   _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#   _partial_: true
#   T_0: 1
#   T_mult: 2
#   eta_min: 0.00001

# scheduler_discriminator:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#   _partial_: true
#   T_0: 1
#   T_mult: 2
#   eta_min: 0.0001
