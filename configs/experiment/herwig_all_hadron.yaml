# @package _global_

# start the environment
# conda-start torch

# to execute this experiment run:
# python hadml/train.py experiment=herwig_all_hadron
## to add a logger
# python hadml/train.py experiment=herwig_all_hadron logger=wandb

## with training techniques
# python hadml/train.py experiment=herwig_all_hadron logger=wandb +trainer.gradient_clip_val=0.5

defaults:
  - override /datamodule: herwig.yaml
  - override /model: cond_particle_gan_separate_embedding.yaml
  - override /callbacks: default.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
task_name: "herwigAllhadron"
tags: ["herwig", "allhadrons"]

seed: 12345

trainer:
  max_epochs: 25
  check_val_every_n_epoch: 1
  val_check_interval: 0.5

callbacks:
  model_checkpoint:
    monitor: val/swd
    # monitor: "val/min_avg_swd"
    mode: "min"
    save_top_k: 5
    save_last: True
  
## override /datamodule:
datamodule:
  batch_size: 40_000
  pin_memory: True
  core_dataset:
    train_val_test_split: [ 0.97, 0.005, 0.025 ]
    frac_data_used: 1.0
    num_used_hadron_types: 40
    fname: allHadrons_with_quark.dat
    cache_dir: /global/cfs/projectdirs/m3246/hadml/processed_data_nominal



## override /model:
model:
  noise_dim: 64
  # loss_type: "wasserstein" ## "wasserstein" or "bce" or "ls"
  loss_type: "bce"
  wasserstein_reg: 0.0
  r1_reg: 200.0

  ## how often the optimizers will be used.
  num_critics: 1
  num_gen: 1

  generator:
    input_dim: ${eval:${model.noise_dim}+${model.cond_info_dim}}
    hidden_dims: ${gen_list:1000,4} # = [1000,] * 4

  discriminator:
    hidden_dims: ${gen_list:1000,4} # = [1000,] * 4
    dropout: 0
